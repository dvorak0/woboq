<dec f='codebrowser/gtsam/linear/GaussianFactorGraph.h' l='368' type='VectorValues gtsam::GaussianFactorGraph::optimizeGradientSearch() const'/>
<doc f='codebrowser/gtsam/linear/GaussianFactorGraph.h' l='344'>/** Optimize along the gradient direction, with a closed-form computation to perform the line
     *  search.  The gradient is computed about \f$ \delta x=0 \f$.
     *
     *  This function returns \f$ \delta x \f$ that minimizes a reparametrized problem.  The error
     *  function of a GaussianBayesNet is
     *
     *  \f[ f(\delta x) = \frac{1}{2} |R \delta x - d|^2 = \frac{1}{2}d^T d - d^T R \delta x +
     *  \frac{1}{2} \delta x^T R^T R \delta x \f]
     *
     *  with gradient and Hessian
     *
     *  \f[ g(\delta x) = R^T(R\delta x - d), \qquad G(\delta x) = R^T R. \f]
     *
     *  This function performs the line search in the direction of the gradient evaluated at \f$ g =
     *  g(\delta x = 0) \f$ with step size \f$ \alpha \f$ that minimizes \f$ f(\delta x = \alpha g)
     *  \f$:
     *
     *  \f[ f(\alpha) = \frac{1}{2} d^T d + g^T \delta x + \frac{1}{2} \alpha^2 g^T G g \f]
     *
     *  Optimizing by setting the derivative to zero yields \f$ \hat \alpha = (-g^T g) / (g^T G g)
     *  \f$.  For efficiency, this function evaluates the denominator without computing the Hessian
     *  \f$ G \f$, returning
     *
     *  \f[ \delta x = \hat\alpha g = \frac{-g^T g}{(R g)^T(R g)} \f] */</doc>
<use f='codebrowser/gtsam/linear/GaussianBayesNet.cpp' l='80' u='c' c='_ZNK5gtsam16GaussianBayesNet22optimizeGradientSearchEv'/>
<use f='codebrowser/gtsam/linear/GaussianBayesTree.cpp' l='76' u='c' c='_ZNK5gtsam17GaussianBayesTree22optimizeGradientSearchEv'/>
<def f='codebrowser/gtsam/linear/GaussianFactorGraph.cpp' l='381' ll='407' type='VectorValues gtsam::GaussianFactorGraph::optimizeGradientSearch() const'/>
<doc f='codebrowser/gtsam/linear/GaussianFactorGraph.cpp' l='380'>/* ************************************************************************* */</doc>
